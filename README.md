## Long Document Summarization

## Problem Statement

Develop a customized LLM model that can generate a summary of a given Research document.

## Proposed Solution 

- Preprocess the article's context
- Generate an extractive summary using sentence-transformer
- Generate a prompt dataset
- Fine-tune the [Falcon Model (1B)](tiiuae/falcon-rw-1b)
- Evaluate the Model output
- Serve the finetuned LLM model using [VLLM](https://github.com/vllm-project/vllm)
- Containerize the inference pipeline and deploy the APIâ€™s endpoint with [Streamlit](https://streamlit.io/) integrated UI

## Arxiv dataset for summarization

Dataset for summarization of long documents.\
Huggingface Link - [ccdv/arxiv-summarization](https://huggingface.co/datasets/ccdv/arxiv-summarization)

### Data Fields

- `id`: paper id
- `article`: a string containing the body of the paper
- `abstract`: a string containing the abstract of the paper

### **Part 1 - Preprocessing Steps**


- **Tokenization**: The input document is tokenized into sentences using NLTK's sentence tokenizer. This step divides the document into manageable segments.

- **Generate Contextual Embeddings**: Sentence Transformers are used to create contextual embeddings for each sentence. [paraphrase-MiniLM-L3-v2](https://huggingface.co/sentence-transformers/paraphrase-MiniLM-L3-v2) has been used due to its speed and efficiency, as indicated in the [Sentence Transformers documentation](https://www.sbert.net/docs/pretrained_models.html)
- **Sentence Clustering**: Sentences are clustered using K-means which groups similar sentences together, helping to identify important information from the article.

- **Representative Sentence Selection**: The sentence closest to the centroid of each cluster is chosen as the representation sentence for that group. This ensures that the most informative sentence in each topic cluster is included in the summary.

- **Create Extractive Summary**: The extractive summary is generated by combining these sentences
- **Generating the Prompt for finetuning**: The extractive summary obtained in the previous step is combined with the document's abstract. This combined text serves as the prompt for the language model.

### **Part 2 - Finetuning Model**
- Download [Falcon Model (1B)](tiiuae/falcon-rw-1b) Model Source - Paper: https://arxiv.org/abs/2306.01116
- Load the model in 4bit with nf4 Quantization Technique using **Bitsandbytes** Paper - https://arxiv.org/pdf/2305.14314.pdf
- Import PEFT and pass the Lora config to finetune the Falcon model with the generated prompt
- Finetune the model and save the adapter/checkpoint to the huggingface hub
- Merge the LORA adapter and push the model to huggingface hub

### **Train/Loss Graph**

![train_loss](https://github.com/GouthamVicky/LLM-LongDoc-Summary/assets/65328702/d96c87d5-f41b-4deb-b5c6-bbf81d902a24)



### Model Card

#### Model Card Link - [GouthamVignesh/falcon-arxiv-long-summary-1B](https://huggingface.co/GouthamVignesh/falcon-arxiv-long-summary-1B)
###  Model Card Link - [GouthamVignesh/falcon-long-summary-ckpt](https://huggingface.co/GouthamVignesh/falcon-long-summary-ckpt)

### Model Description
Finetuned Falcon-1B model for document summarization task

- **Model type:** CasualLM
- **Language(s) (NLP):** EN
- **Finetuned from model:** [tiiuae/falcon-rw-1b](https://huggingface.co/tiiuae/falcon-rw-1b)

### **Part 2 - Evaluvaton**
- Load Dataset and Initialize Lists.
- Initialize Sentence Transformer Model.
- Calculate BLEU, ROUGE, and Semantic Similarity Scores.
- Iterate and Evaluate Each Example.
- Calculate Average BLEU, ROUGE, and Semantic Similarity Scores.
- Combine Scores into a Hybrid Score.
- Interpret the Results.

## **Part 3 - Inference Comparision**
- Inference comparison between huggingface generation pipeline and VLLM Inference
  ![download](https://github.com/GouthamVicky/LLM-LongDoc-Summary/assets/65328702/d3c2a661-7b80-46fb-b2ab-68080d530cd8)

## **Part 4 - Streamlit Model Deployment**
- Build a docker Image to run the streamlit application 
