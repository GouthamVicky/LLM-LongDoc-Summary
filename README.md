## Research Paper Summarization

## Problem Statement

Develop a customized LLM model that can generate a summary of a given document.

## Proposed Solution 

- Preprocess the article's context
- Generate an extractive summary using sentence-transformer
- Generate a prompt dataset
- Fine-tune the [Falcon Model (1B)](tiiuae/falcon-rw-1b)
- Evaluate the Model output
- Serve the finetuned LLM model using [VLLM](https://github.com/vllm-project/vllm)
- Containerize the inference pipeline and deploy the APIâ€™s endpoint with [Streamlit](https://streamlit.io/) integrated UI

## Arxiv dataset for summarization

Dataset for summarization of long documents.\
Huggingface Link - [ccdv/arxiv-summarization](https://huggingface.co/datasets/ccdv/arxiv-summarization)

### Data Fields

- `id`: paper id
- `article`: a string containing the body of the paper
- `abstract`: a string containing the abstract of the paper

### **Part 1 - Preprocessing Steps**


- **Tokenization**: The input document is tokenized into sentences using NLTK's sentence tokenizer. This step divides the document into manageable segments.

- **Generate Contextual Embeddings**: Sentence Transformers are used to create contextual embeddings for each sentence. [paraphrase-MiniLM-L3-v2](https://huggingface.co/sentence-transformers/paraphrase-MiniLM-L3-v2) has been used due to its speed and efficiency, as indicated in the [Sentence Transformers documentation](https://www.sbert.net/docs/pretrained_models.html)
- **Sentence Clustering**: Sentences are clustered using K-means which groups similar sentences together, helping to identify important information from the article.

- **Representative Sentence Selection**: The sentence closest to the centroid of each cluster is chosen as the representation sentence for that group. This ensures that the most informative sentence in each topic cluster is included in the summary.

- **Create Extractive Summary**: The extractive summary is generated by combining these sentences
- **Generating the Prompt for finetuning**: The extractive summary obtained in the previous step is combined with the document's abstract. This combined text serves as the prompt for the language model.
